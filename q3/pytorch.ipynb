{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Techniques With PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(image):\n",
    "    # Horizontal and vertical flips\n",
    "    flipped_image_x = TF.hflip(image)\n",
    "    flipped_image_y = TF.vflip(image)\n",
    "\n",
    "    # Saturation adjustments\n",
    "    saturated_image_1 = TF.adjust_saturation(image, 0.5)\n",
    "    saturated_image_2 = TF.adjust_saturation(image, 1.5)\n",
    "\n",
    "    # Brightness adjustments\n",
    "    brightened_image_1 = TF.adjust_brightness(image, 0.5)\n",
    "    brightened_image_2 = TF.adjust_brightness(image, 2.0)\n",
    "\n",
    "    # Contrast adjustments\n",
    "    contrast_image_1 = TF.adjust_contrast(image, 0.5)\n",
    "    contrast_image_2 = TF.adjust_contrast(image, 2.0)\n",
    "\n",
    "    # Shear transformations using torchvision.transforms.functional.affine\n",
    "    translated_image_10_20 = TF.affine(image, angle=0, translate=(10, 20), scale=1.0, shear=(0, 0))\n",
    "    translated_image_30_50 = TF.affine(image, angle=0, translate=(30, 50), scale=1.0, shear=(0, 0))\n",
    "\n",
    "    # Rotations\n",
    "    rotated_image_90 = TF.rotate(image, 90)\n",
    "    rotated_image_180 = TF.rotate(image, 180)\n",
    "\n",
    "    return {\n",
    "        'flipped_horizontal': flipped_image_x,\n",
    "        'flipped_vertical': flipped_image_y,\n",
    "        'saturated_0.5': saturated_image_1,\n",
    "        'saturated_1.5': saturated_image_2,\n",
    "        'brightened_0.5': brightened_image_1,\n",
    "        'brightened_2.0': brightened_image_2,\n",
    "        'contrast_0.5': contrast_image_1,\n",
    "        'contrast_2.0': contrast_image_2,\n",
    "        'translated_10_20': translated_image_10_20,\n",
    "        'translated_30_50': translated_image_30_50,\n",
    "        'rotated_90': rotated_image_90,\n",
    "        'rotated_180': rotated_image_180\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, original_name, suffix, output_dir):\n",
    "    base_name, ext = os.path.splitext(original_name)\n",
    "    new_name = f\"{base_name}_{suffix}{ext}\"\n",
    "    image.save(os.path.join(output_dir, new_name))\n",
    "\n",
    "input_dir = '../images'\n",
    "output_dir = 'pytorch_augmented_images'\n",
    "\n",
    "image_files = [f for f in os.listdir(input_dir) if f.endswith(('.jpeg'))]\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(input_dir, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    transformed_images = apply_transformations(image)\n",
    "\n",
    "    for suffix, img in transformed_images.items():\n",
    "        save_image(img, image_file, suffix, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
