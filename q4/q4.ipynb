{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Augmentation Techniques from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saturation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_saturation(image, saturation_factor):\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hsv_image = hsv_image.astype(np.float32)\n",
        "    h, s, v = cv2.split(hsv_image)\n",
        "    s *= saturation_factor\n",
        "    s = np.clip(s, 0, 255)\n",
        "    hsv_image = cv2.merge([h, s, v])\n",
        "    hsv_image = hsv_image.astype(np.uint8)\n",
        "    adjusted_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    return adjusted_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contrast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        " \n",
        "def adjust_contrast(image, factor):\n",
        "    f_image = image.astype(np.float32)\n",
        "    mean = np.mean(f_image, axis=(0, 1), keepdims=True)\n",
        "    adjusted_image = mean + factor * (f_image - mean)\n",
        "    adjusted_image = np.clip(adjusted_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return adjusted_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Brightness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_brightness(image, brightness_factor):\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv_image)\n",
        "    v = np.clip(v * brightness_factor, 0, 255).astype(np.uint8)\n",
        "    adjusted_hsv_image = cv2.merge([h, s, v])\n",
        "    adjusted_image = cv2.cvtColor(adjusted_hsv_image, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    return adjusted_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flip_image(image, flip_type='horizontal'):\n",
        "    if flip_type == 'horizontal':\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "    elif flip_type == 'vertical':\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid flip_type. Choose from 'horizontal' or 'vertical'.\")\n",
        "\n",
        "    return flipped_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rotate_image(image, angle):\n",
        "    original_height, original_width = image.shape[:2]\n",
        "    center = (original_width // 2, original_height // 2)\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, (original_width, original_height))\n",
        "\n",
        "    return rotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_image(image, tx, ty):\n",
        "    original_height, original_width = image.shape[:2]\n",
        "    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    translated_image = cv2.warpAffine(image, translation_matrix, (original_width, original_height))\n",
        "\n",
        "    return translated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_image(image, original_name, suffix, output_dir):\n",
        "    base_name, ext = os.path.splitext(original_name)\n",
        "    new_name = f\"{base_name}_{suffix}{ext}\"\n",
        "    cv2.imwrite(os.path.join(output_dir, new_name), image)\n",
        "\n",
        "input_dir = '../images'\n",
        "output_dir = 'augmented_images'\n",
        "\n",
        "image_files = [f for f in os.listdir(input_dir) if f.endswith(('.jpeg'))]\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(input_dir, image_file)\n",
        "    pixels = cv2.imread(image_path)\n",
        "    transformed_images = {}\n",
        "\n",
        "    # Transformations\n",
        "    transformed_images['saturated_0.5'] = adjust_saturation(pixels, 0.5)\n",
        "    transformed_images['saturated_1.5'] = adjust_saturation(pixels, 1.5)\n",
        "    transformed_images['flipped_horizontal'] = flip_image(pixels, 'horizontal')\n",
        "    transformed_images['flipped_vertical'] = flip_image(pixels, 'vertical')\n",
        "    transformed_images['contrast_0.5'] = adjust_contrast(pixels, 0.5)\n",
        "    transformed_images['contrast_2.0'] = adjust_contrast(pixels, 2.0)\n",
        "    transformed_images['translated_10_20'] = translate_image(pixels, 10, 20)\n",
        "    transformed_images['translated_30_50'] = translate_image(pixels, 30, 50)\n",
        "    transformed_images['brightened_0.5'] = adjust_brightness(pixels, 0.5)\n",
        "    transformed_images['brightened_2.0'] = adjust_brightness(pixels, 2.0)\n",
        "    transformed_images['rotated_90'] = rotate_image(pixels, 90)\n",
        "    transformed_images['rotated_180'] = rotate_image(pixels, 180)\n",
        "\n",
        "    for suffix, transformed_image in transformed_images.items():\n",
        "        save_image(transformed_image, image_file, suffix, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison\n",
        "\n",
        "To compare we compared our augmented version with the pytorch augmentations by computing the correltaions of each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir1 = 'augmented_images'\n",
        "dir2 = '../q3/pytorch_augmented_images'\n",
        "\n",
        "files1 = [f for f in os.listdir(dir1) if f.endswith(('.jpeg', '.png', '.jpg'))]\n",
        "files2 = [f for f in os.listdir(dir2) if f.endswith(('.jpeg', '.png', '.jpg'))]\n",
        "\n",
        "common_files = set(files1).intersection(set(files2))\n",
        "\n",
        "def load_and_preprocess_image(image_path, size=(256, 256)):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, size)\n",
        "    return image\n",
        "\n",
        "correlations = {}\n",
        "\n",
        "for fname in common_files:\n",
        "    img1 = load_and_preprocess_image(os.path.join(dir1, fname))\n",
        "    img2 = load_and_preprocess_image(os.path.join(dir2, fname))\n",
        "    \n",
        "    img1_flat = img1.flatten()\n",
        "    img2_flat = img2.flatten()\n",
        "    \n",
        "    corr, _ = pearsonr(img1_flat, img2_flat)\n",
        "    correlations[fname] = corr\n",
        "\n",
        "with open(\"correlations.txt\", \"w\") as f:\n",
        "    for fname, corr in correlations.items():\n",
        "        f.write(f\"{fname}: {corr}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "**Below are the correlation values for the 'dl_book_colour' image:**\n",
        "\n",
        "buddha_colour_saturated_0.5.jpeg: 0.9922868238804489\n",
        "\n",
        "buddha_colour_saturated_1.5.jpeg: 0.9977504243945872\n",
        "\n",
        "buddha_colour_brightened_0.5.jpeg: 0.9907473076256899\n",
        "\n",
        "buddha_colour_brightened_2.0.jpeg: 0.9511899341734003\n",
        "\n",
        "buddha_colour_translated_10_20.jpeg: 0.9969093588960934\n",
        "\n",
        "buddha_colour_translated_30_50.jpeg: 0.9980011965825497\n",
        "\n",
        "buddha_colour_contrast_0.5.jpeg: 0.9364953558394352\n",
        "\n",
        "buddha_colour_contrast_2.0.jpeg: 0.9483491425528454\n",
        "\n",
        "buddha_colour_flipped_horizontal.jpeg: 0.999781662014373\n",
        "\n",
        "buddha_colour_flipped_vertical.jpeg: 0.9997867869564403\n",
        "\n",
        "buddha_colour_rotated_90.jpeg: 0.9983660468219826\n",
        "\n",
        "buddha_colour_rotated_180.jpeg: 0.9784485376802984\n",
        "\n",
        "**We noted that ur brightening and contrast functions were just slightly off from the pytorch standard.**\n",
        "\n",
        "**The rest of the results can be viewed below:**\n",
        "\n",
        "buddha_colour_saturated_1.5.jpeg: 0.9977504243945872\n",
        "\n",
        "buddha_colour_brightened_2.0.jpeg: 0.9511899341734003\n",
        "\n",
        "tajin_colour_flipped_vertical.jpeg: 0.9999094151098544\n",
        "\n",
        "buddha_colour_brightened_0.5.jpeg: 0.9907473076256899\n",
        "\n",
        "macbook_colour_saturated_0.5.jpeg: 0.9933166261379309\n",
        "\n",
        "dl_book_colour_brightened_0.5.jpeg: 0.9920402746110856\n",
        "\n",
        "lp_world_colour_contrast_2.0.jpeg: 0.9554330398169275\n",
        "\n",
        "lp_world_colour_contrast_0.5.jpeg: 0.9378049487216751\n",
        "\n",
        "dl_book_colour_translated_30_50.jpeg: 0.9978899324388919\n",
        "\n",
        "dl_book_colour_rotated_180.jpeg: 0.9719812290410986\n",
        "\n",
        "tajin_colour_saturated_1.5.jpeg: 0.9960967710313156\n",
        "\n",
        "lp_world_colour_saturated_0.5.jpeg: 0.9856090957261219\n",
        "\n",
        "dl_book_colour_translated_10_20.jpeg: 0.9973112609268913\n",
        "\n",
        "macbook_colour_flipped_horizontal.jpeg: 0.9998551347813078\n",
        "\n",
        "tajin_colour_brightened_0.5.jpeg: 0.9950185136669101\n",
        "\n",
        "dl_book_colour_saturated_0.5.jpeg: 0.988989217010966\n",
        "\n",
        "dl_book_colour_saturated_1.5.jpeg: 0.9950324001742756\n",
        "\n",
        "buddha_colour_translated_10_20.jpeg: 0.9969093588960934\n",
        "\n",
        "tajin_colour_rotated_180.jpeg: 0.9738250354949266\n",
        "\n",
        "dl_book_colour_brightened_2.0.jpeg: 0.9928663316876847\n",
        "\n",
        "dl_book_colour_contrast_0.5.jpeg: 0.9364434126765929\n",
        "\n",
        "lp_world_colour_flipped_vertical.jpeg: 0.9998935633304774\n",
        "\n",
        "tajin_colour_brightened_2.0.jpeg: 0.9942277152099921\n",
        "\n",
        "macbook_colour_translated_10_20.jpeg: 0.9977512812022342\n",
        "\n",
        "macbook_colour_rotated_90.jpeg: 0.9989192049920855\n",
        "\n",
        "buddha_colour_saturated_0.5.jpeg: 0.9922868238804489\n",
        "\n",
        "buddha_colour_contrast_0.5.jpeg: 0.9364953558394352\n",
        "\n",
        "macbook_colour_brightened_0.5.jpeg: 0.9931289235920808\n",
        "\n",
        "dl_book_colour_flipped_horizontal.jpeg: 0.9998589743245696\n",
        "\n",
        "lp_world_colour_translated_30_50.jpeg: 0.99856915002376\n",
        "\n",
        "lp_world_colour_brightened_2.0.jpeg: 0.9862050512624521\n",
        "\n",
        "macbook_colour_saturated_1.5.jpeg: 0.9975407402527472\n",
        "\n",
        "macbook_colour_translated_30_50.jpeg: 0.9984287964812518\n",
        "\n",
        "lp_world_colour_flipped_horizontal.jpeg: 0.9998822279293001\n",
        "\n",
        "buddha_colour_flipped_horizontal.jpeg: 0.9997816620143739\n",
        "\n",
        "macbook_colour_rotated_180.jpeg: 0.9930033977523309\n",
        "\n",
        "dl_book_colour_flipped_vertical.jpeg: 0.999872043316193\n",
        "\n",
        "lp_world_colour_saturated_1.5.jpeg: 0.9936735919889206\n",
        "\n",
        "tajin_colour_saturated_0.5.jpeg: 0.9920134069861877\n",
        "\n",
        "tajin_colour_translated_30_50.jpeg: 0.9986058869769754\n",
        "\n",
        "tajin_colour_flipped_horizontal.jpeg: 0.99990253174572\n",
        "\n",
        "tajin_colour_contrast_0.5.jpeg: 0.9428165103496648\n",
        "\n",
        "buddha_colour_contrast_2.0.jpeg: 0.9483491425528454\n",
        "\n",
        "dl_book_colour_contrast_2.0.jpeg: 0.9537274104273332\n",
        "\n",
        "lp_world_colour_rotated_180.jpeg: 0.9876592266562696\n",
        "\n",
        "lp_world_colour_rotated_90.jpeg: 0.9963340101613245\n",
        "\n",
        "macbook_colour_flipped_vertical.jpeg: 0.9998792867284101\n",
        "\n",
        "tajin_colour_translated_10_20.jpeg: 0.998267272422891\n",
        "\n",
        "dl_book_colour_rotated_90.jpeg: 0.9906991010261856\n",
        "\n",
        "lp_world_colour_translated_10_20.jpeg: 0.9981167151546495\n",
        "\n",
        "tajin_colour_rotated_90.jpeg: 0.992473227857493\n",
        "\n",
        "macbook_colour_brightened_2.0.jpeg: 0.999410850776757\n",
        "\n",
        "buddha_colour_rotated_90.jpeg: 0.9983660468219826\n",
        "\n",
        "lp_world_colour_brightened_0.5.jpeg: 0.9945058161129994\n",
        "\n",
        "buddha_colour_rotated_180.jpeg: 0.9784485376802984\n",
        "\n",
        "tajin_colour_contrast_2.0.jpeg: 0.9709173749393627\n",
        "\n",
        "macbook_colour_contrast_0.5.jpeg: 0.9477140754317523\n",
        "\n",
        "buddha_colour_flipped_vertical.jpeg: 0.9997867869564403\n",
        "\n",
        "macbook_colour_contrast_2.0.jpeg: 0.9342006169375852\n",
        "\n",
        "buddha_colour_translated_30_50.jpeg: 0.9980011965825497\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
