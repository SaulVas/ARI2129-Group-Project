{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Augmentation Techniques from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HSL Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rgb_to_hsl(r, g, b):\n",
        "    r, g, b = r / 255.0, g / 255.0, b / 255.0\n",
        "    max_c = np.maximum(np.maximum(r, g), b)\n",
        "    min_c = np.minimum(np.minimum(r, g), b)\n",
        "    lightness = (max_c + min_c) / 2.0\n",
        "\n",
        "    saturation = np.zeros_like(lightness)\n",
        "    hue = np.zeros_like(lightness)\n",
        "\n",
        "    mask = max_c != min_c\n",
        "    dif = max_c - min_c\n",
        "    saturation[mask] = np.where(lightness[mask] > 0.5, dif[mask] / (2.0 - max_c[mask] - min_c[mask]), dif[mask] / (max_c[mask] + min_c[mask]))\n",
        "\n",
        "    mask_r = (max_c == r) & mask\n",
        "    mask_g = (max_c == g) & mask\n",
        "    mask_b = (max_c == b) & mask\n",
        "\n",
        "    hue[mask_r] = ((g[mask_r] - b[mask_r]) / dif[mask_r] + (g[mask_r] < b[mask_r]) * 6) % 6\n",
        "    hue[mask_g] = ((b[mask_g] - r[mask_g]) / dif[mask_g] + 2) % 6\n",
        "    hue[mask_b] = ((r[mask_b] - g[mask_b]) / dif[mask_b] + 4) % 6\n",
        "    hue /= 6\n",
        "\n",
        "    return hue, saturation, lightness\n",
        "\n",
        "def hsl_to_rgb(hue, saturation, lightness):\n",
        "    def hue_to_rgb(p, q, t):\n",
        "        t = np.where(t < 0, t + 1, t)\n",
        "        t = np.where(t > 1, t - 1, t)\n",
        "        return np.where(t < 1 / 6, p + (q - p) * 6 * t, \n",
        "                        np.where(t < 1 / 2, q,\n",
        "                                 np.where(t < 2 / 3, p + (q - p) * (2 / 3 - t) * 6, p)))\n",
        "\n",
        "    q = np.where(lightness < 0.5, lightness * (1 + saturation), lightness + saturation - lightness * saturation)\n",
        "    p = 2 * lightness - q\n",
        "    r = hue_to_rgb(p, q, hue + 1 / 3)\n",
        "    g = hue_to_rgb(p, q, hue)\n",
        "    b = hue_to_rgb(p, q, hue - 1 / 3)\n",
        "    return (r * 255).astype(np.uint8), (g * 255).astype(np.uint8), (b * 255).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saturation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_saturation(image, saturation_factor):\n",
        "    r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
        "    hue, saturation, lightness = rgb_to_hsl(r, g, b)\n",
        "    saturation *= saturation_factor\n",
        "    saturation = np.clip(saturation, 0, 1)\n",
        "\n",
        "    r, g, b = hsl_to_rgb(hue, saturation, lightness)\n",
        "    new_pixels = np.stack([r, g, b], axis=-1)\n",
        "\n",
        "    new_image = Image.fromarray(new_pixels, 'RGB')\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contrast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        " \n",
        "def adjust_contrast(pixels, factor):\n",
        "    mean = np.mean(pixels)\n",
        "    \n",
        "    new_pixels = mean + (pixels - mean) * factor\n",
        "    new_pixels = np.clip(new_pixels, 0, 255)\n",
        "    \n",
        "    new_pixels = new_pixels.astype(np.uint8)\n",
        "    \n",
        "    new_image = Image.fromarray(new_pixels)\n",
        "    return new_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Brightness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_brightness(image, brightness_factor):\n",
        "    r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
        "    hue, saturation, lightness = rgb_to_hsl(r, g, b)\n",
        "    lightness *= brightness_factor\n",
        "    lightness = np.clip(lightness, 0, 1)\n",
        "\n",
        "    r, g, b = hsl_to_rgb(hue, saturation, lightness)\n",
        "    new_pixels = np.stack([r, g, b], axis=-1)\n",
        "\n",
        "    new_image = Image.fromarray(new_pixels, 'RGB')\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flip_image(image, flip_type='horizontal'):\n",
        "    if flip_type == 'horizontal':\n",
        "        flipped_pixels = np.fliplr(image)\n",
        "    elif flip_type == 'vertical':\n",
        "        flipped_pixels = np.flipud(image)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid flip_type. Choose from 'horizontal' or 'vertical'.\")\n",
        "    \n",
        "    flipped_image = Image.fromarray(flipped_pixels)\n",
        "    return flipped_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rotate_image(pixels, angle):\n",
        "    angle_rad = np.deg2rad(angle)\n",
        "    \n",
        "    original_height, original_width, _ = pixels.shape\n",
        "\n",
        "    center_x, center_y = original_width // 2, original_height // 2\n",
        "    \n",
        "    y_indices, x_indices = np.meshgrid(np.arange(original_height), np.arange(original_width), indexing='ij')\n",
        "    \n",
        "    x_indices_flat = x_indices.flatten()\n",
        "    y_indices_flat = y_indices.flatten()\n",
        "    \n",
        "    relative_x = x_indices_flat - center_x\n",
        "    relative_y = y_indices_flat - center_y\n",
        "    \n",
        "    orig_x_flat = (relative_x * np.cos(angle_rad) + relative_y * np.sin(angle_rad)).astype(int) + center_x\n",
        "    orig_y_flat = (-relative_x * np.sin(angle_rad) + relative_y * np.cos(angle_rad)).astype(int) + center_y\n",
        "    valid_mask = (orig_x_flat >= 0) & (orig_x_flat < original_width) & (orig_y_flat >= 0) & (orig_y_flat < original_height)\n",
        "    \n",
        "    rotated_image = np.zeros_like(pixels)\n",
        "    rotated_image[y_indices_flat[valid_mask], x_indices_flat[valid_mask]] = pixels[orig_y_flat[valid_mask], orig_x_flat[valid_mask]]\n",
        "    \n",
        "    new_image = Image.fromarray(rotated_image)\n",
        "\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_image(pixels, tx, ty):\n",
        "    original_height, original_width, _ = pixels.shape\n",
        "\n",
        "    translated_image = np.zeros_like(pixels)\n",
        "\n",
        "    y_indices, x_indices = np.meshgrid(np.arange(original_height), np.arange(original_width), indexing='ij')\n",
        "\n",
        "    translated_x = x_indices - tx\n",
        "    translated_y = y_indices - ty\n",
        "\n",
        "    valid_mask = (translated_x >= 0) & (translated_x < original_width) & (translated_y >= 0) & (translated_y < original_height)\n",
        "\n",
        "    translated_image[y_indices[valid_mask], x_indices[valid_mask]] = pixels[translated_y[valid_mask], translated_x[valid_mask]]\n",
        "    new_image = Image.fromarray(translated_image)\n",
        "\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_image(image, original_name, suffix, output_dir):\n",
        "    base_name, ext = os.path.splitext(original_name)\n",
        "    new_name = f\"{base_name}_{suffix}{ext}\"\n",
        "    image.save(os.path.join(output_dir, new_name))\n",
        "\n",
        "input_dir = '../images'\n",
        "output_dir = 'augmented_images'\n",
        "\n",
        "image_files = [f for f in os.listdir(input_dir) if f.endswith(('.jpeg'))]\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(input_dir, image_file)\n",
        "    image = Image.open(image_path)\n",
        "    pixels = np.array(image)\n",
        "    transformed_images = {}\n",
        "\n",
        "    # Transformations\n",
        "    transformed_images['saturated_0.5'] = adjust_saturation(pixels, 0.5)\n",
        "    transformed_images['saturated_1.5'] = adjust_saturation(pixels, 1.5)\n",
        "    transformed_images['flipped_horizontal'] = flip_image(pixels, 'horizontal')\n",
        "    transformed_images['flipped_vertical'] = flip_image(pixels, 'vertical')\n",
        "    transformed_images['contrast_0.5'] = adjust_contrast(pixels, 0.5)\n",
        "    transformed_images['contrast_2.0'] = adjust_contrast(pixels, 2.0)\n",
        "    transformed_images['translated_10_20'] = translate_image(pixels, 10, 20)\n",
        "    transformed_images['translated_30_50'] = translate_image(pixels, 30, 50)\n",
        "    transformed_images['brightened_0.5'] = adjust_brightness(pixels, 0.5)\n",
        "    transformed_images['brightened_2.0'] = adjust_brightness(pixels, 2.0)\n",
        "    transformed_images['rotated_90'] = rotate_image(pixels, 90)\n",
        "    transformed_images['rotated_180'] = rotate_image(pixels, 180)\n",
        "\n",
        "    for suffix, img in transformed_images.items():\n",
        "        save_image(img, image_file, suffix, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison\n",
        "\n",
        "To compare we compared our augmented version with the pytorch augmentations by computing the correltaions of each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir1 = 'augmented_images'\n",
        "dir2 = '../q3/pytorch_augmented_images'\n",
        "\n",
        "files1 = [f for f in os.listdir(dir1) if f.endswith(('.jpeg', '.png', '.jpg'))]\n",
        "files2 = [f for f in os.listdir(dir2) if f.endswith(('.jpeg', '.png', '.jpg'))]\n",
        "\n",
        "common_files = set(files1).intersection(set(files2))\n",
        "\n",
        "def load_and_preprocess_image(image_path, size=(256, 256)):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, size)\n",
        "    return image\n",
        "\n",
        "correlations = {}\n",
        "\n",
        "for fname in common_files:\n",
        "    img1 = load_and_preprocess_image(os.path.join(dir1, fname))\n",
        "    img2 = load_and_preprocess_image(os.path.join(dir2, fname))\n",
        "    \n",
        "    img1_flat = img1.flatten()\n",
        "    img2_flat = img2.flatten()\n",
        "    \n",
        "    corr, _ = pearsonr(img1_flat, img2_flat)\n",
        "    correlations[fname] = corr\n",
        "\n",
        "with open(\"correlations.txt\", \"w\") as f:\n",
        "    for fname, corr in correlations.items():\n",
        "        f.write(f\"{fname}: {corr}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "**Below are the correlation values for the 'dl_book_colour' image:**\n",
        "\n",
        "dl_book_colour_translated_10_20.jpeg: 0.9999999999999358\n",
        "\n",
        "dl_book_colour_translated_30_50.jpeg: 1.0\n",
        "\n",
        "dl_book_colour_rotated_90.jpeg: 0.8188053831563114\n",
        "\n",
        "dl_book_colour_rotated_180.jpeg: 0.9708106728005539\n",
        "\n",
        "dl_book_colour_contrast_0.5.jpeg: 0.9898123953527982\n",
        "\n",
        "dl_book_colour_contrast_2.0.jpeg: 0.9989658472449435\n",
        "\n",
        "dl_book_colour_flipped_vertical.jpeg: 1.0\n",
        "\n",
        "dl_book_colour_flipped_horizontal.jpeg: 0.9999999999998306\n",
        "\n",
        "dl_book_colour_saturated_0.5.jpeg: 0.9945072050758152\n",
        "\n",
        "dl_book_colour_saturated_1.5.jpeg: 0.9979227760629376\n",
        "\n",
        "dl_book_colour_brightened_0.5.jpeg: 0.990413991311965\n",
        "\n",
        "dl_book_colour_brightened_2.0.jpeg: 0.9874516361445039\n",
        "\n",
        "**We noticed that our 90 degree rotation was a bit low, and this is because we rotated clockwise, while pytorch rotates anticlockwise.**\n",
        "\n",
        "**The rest of the results can be viewed below:**\n",
        "\n",
        "buddha_colour_saturated_1.5.jpeg: 0.9992202473780265\n",
        "\n",
        "buddha_colour_brightened_2.0.jpeg: 0.8231497973963382\n",
        "\n",
        "tajin_colour_flipped_vertical.jpeg: 1.0\n",
        "\n",
        "buddha_colour_brightened_0.5.jpeg: 0.9529734702794235\n",
        "\n",
        "macbook_colour_saturated_0.5.jpeg: 0.9966796913973659\n",
        "\n",
        "dl_book_colour_brightened_0.5.jpeg: 0.990413991311965\n",
        "\n",
        "lp_world_colour_contrast_2.0.jpeg: 0.9989867197697653\n",
        "\n",
        "lp_world_colour_contrast_0.5.jpeg: 0.9938766146472087\n",
        "\n",
        "dl_book_colour_translated_30_50.jpeg: 1.0\n",
        "\n",
        "dl_book_colour_rotated_180.jpeg: 0.9708106728005539\n",
        "\n",
        "tajin_colour_saturated_1.5.jpeg: 0.998468635682406\n",
        "\n",
        "lp_world_colour_saturated_0.5.jpeg: 0.9934060025901192\n",
        "\n",
        "dl_book_colour_translated_10_20.jpeg: 0.9999999999999358\n",
        "\n",
        "macbook_colour_flipped_horizontal.jpeg: 1.0\n",
        "\n",
        "tajin_colour_brightened_0.5.jpeg: 0.933254608590525\n",
        "\n",
        "dl_book_colour_saturated_0.5.jpeg: 0.9945072050758152\n",
        "\n",
        "dl_book_colour_saturated_1.5.jpeg: 0.9979227760629376\n",
        "\n",
        "buddha_colour_translated_10_20.jpeg: 1.0\n",
        "\n",
        "tajin_colour_rotated_180.jpeg: 0.9802611515968178\n",
        "\n",
        "dl_book_colour_brightened_2.0.jpeg: 0.9874516361445039\n",
        "\n",
        "dl_book_colour_contrast_0.5.jpeg: 0.9898123953527982\n",
        "\n",
        "lp_world_colour_flipped_vertical.jpeg: 1.0\n",
        "\n",
        "tajin_colour_brightened_2.0.jpeg: 0.9870058390024795\n",
        "\n",
        "macbook_colour_translated_10_20.jpeg: 1.0\n",
        "\n",
        "macbook_colour_rotated_90.jpeg: 0.8948374303697684\n",
        "\n",
        "buddha_colour_saturated_0.5.jpeg: 0.997701814322556\n",
        "\n",
        "buddha_colour_contrast_0.5.jpeg: 0.9865723109008198\n",
        "\n",
        "macbook_colour_brightened_0.5.jpeg: 0.985161101638036\n",
        "\n",
        "dl_book_colour_flipped_horizontal.jpeg: 0.9999999999998306\n",
        "\n",
        "lp_world_colour_translated_30_50.jpeg: 0.9999999999999125\n",
        "\n",
        "lp_world_colour_brightened_2.0.jpeg: 0.9863999303029463\n",
        "\n",
        "macbook_colour_saturated_1.5.jpeg: 0.9991097991773913\n",
        "\n",
        "macbook_colour_translated_30_50.jpeg: 0.9999999999997352\n",
        "\n",
        "lp_world_colour_flipped_horizontal.jpeg: 1.0\n",
        "\n",
        "buddha_colour_flipped_horizontal.jpeg: 0.9999999999997786\n",
        "\n",
        "macbook_colour_rotated_180.jpeg: 0.9924882113972262\n",
        "\n",
        "dl_book_colour_flipped_vertical.jpeg: 1.0\n",
        "\n",
        "lp_world_colour_saturated_1.5.jpeg: 0.9964012521920073\n",
        "\n",
        "tajin_colour_saturated_0.5.jpeg: 0.997218823960397\n",
        "\n",
        "tajin_colour_translated_30_50.jpeg: 1.0\n",
        "\n",
        "tajin_colour_flipped_horizontal.jpeg: 1.0\n",
        "\n",
        "tajin_colour_contrast_0.5.jpeg: 0.9938478007994349\n",
        "\n",
        "buddha_colour_contrast_2.0.jpeg: 0.9996928743386673\n",
        "\n",
        "dl_book_colour_contrast_2.0.jpeg: 0.9989658472449435\n",
        "\n",
        "lp_world_colour_rotated_180.jpeg: 0.9881375319536626\n",
        "\n",
        "lp_world_colour_rotated_90.jpeg: 0.721684572628958\n",
        "\n",
        "macbook_colour_flipped_vertical.jpeg: 1.0\n",
        "\n",
        "tajin_colour_translated_10_20.jpeg: 0.999999999999815\n",
        "\n",
        "dl_book_colour_rotated_90.jpeg: 0.8188053831563114\n",
        "\n",
        "lp_world_colour_translated_10_20.jpeg: 0.9999999999999132\n",
        "\n",
        "tajin_colour_rotated_90.jpeg: 0.5238218654094873\n",
        "\n",
        "macbook_colour_brightened_2.0.jpeg: 0.9936786676653504\n",
        "\n",
        "buddha_colour_rotated_90.jpeg: 0.9115608128974957\n",
        "\n",
        "lp_world_colour_brightened_0.5.jpeg: 0.9905896515360137\n",
        "\n",
        "buddha_colour_rotated_180.jpeg: 0.981369798342363\n",
        "\n",
        "tajin_colour_contrast_2.0.jpeg: 0.9989200421446731\n",
        "\n",
        "macbook_colour_contrast_0.5.jpeg: 0.9997573823174446\n",
        "\n",
        "buddha_colour_flipped_vertical.jpeg: 0.9999999999997025\n",
        "\n",
        "macbook_colour_contrast_2.0.jpeg: 0.9986668636842797\n",
        "\n",
        "buddha_colour_translated_30_50.jpeg: 0.9999999999997552\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
